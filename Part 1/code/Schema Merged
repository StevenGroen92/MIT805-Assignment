# schema_merged.py
# Collect column names + data types per TLC service and merge to one CSV

from pathlib import Path
import argparse
import pandas as pd
from pyspark.sql import SparkSession

SERVICES = ["yellow", "green", "fhvhv"]

def first_parquet_for(service: str, root: Path) -> Path | None:
    # find first parquet anywhere under root for this service
    files = sorted(root.rglob(f"{service}_tripdata_*.parquet"))
    return files[0] if files else None

def schema_for_file(spark: SparkSession, file_path: Path) -> list[tuple[str, str]]:
    df = spark.read.parquet(str(file_path))
    # (column_name, spark_dtype)
    return [(f.name, f.dataType.simpleString()) for f in df.schema.fields]

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--root", type=str, default=".",
                        help="Folder that contains yellow/, green/, fhvhv/ subfolders")
    parser.add_argument("--out", type=str, default="outputs/schemas_merged.csv",
                        help="Output CSV file path")
    args = parser.parse_args()

    root = Path(args.root).resolve()
    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    spark = (
        SparkSession.builder
        .appName("TLC-Schema-Merged")
        .config("spark.sql.session.timeZone", "UTC")
        .getOrCreate()
    )

    rows = []
    for svc in SERVICES:
        fp = first_parquet_for(svc, root)
        if not fp:
            print(f"{svc}: no files found under {root}")
            continue
        print(f"{svc}: using {fp}")
        schema = schema_for_file(spark, fp)
        for name, dtype in schema:
            rows.append({"service": svc, "column": name, "dtype": dtype})

    spark.stop()

    if not rows:
        print("no schemas collected; nothing to write")
        return

    df = pd.DataFrame(rows).sort_values(["service", "column"]).reset_index(drop=True)
    df.to_csv(out_path, index=False)
    print(f"saved â†’ {out_path}")

if __name__ == "__main__":
    main()
